{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1652be-c06d-4004-bd0f-a5bfa432ed94",
   "metadata": {},
   "source": [
    "# train_unet_simplistic.ipynb\n",
    "This notebook has the analoguous functionality as the  `train.py` file. Use might use this notebook to develop and run the code interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bbb6a93-5a39-4b1b-87b8-b27ed2011c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "# import functions from files that you have defined.\n",
    "# if you are copying these function definitions into the jupyter notebook, make sure to remove the corresponding import commands.\n",
    "from eth_mugs_dataset_simplistic import ETHMugsDataset\n",
    "from utils import IMAGE_SIZE, compute_iou, save_predictions\n",
    "from train_unet_simplistic import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849ac490-068b-4ddf-84eb-12b558b0f9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Model checkpoints will be saved to: ./checkpoints/2025-05-10-09-34-55\n",
      "[INFO]: Train data root: sml_data/datasets/train_data\n",
      "[INFO]: Test data root: sml_data/datasets/test_data\n",
      "[INFO]: Number of training epochs: 10\n",
      "[INFO] Dataset mode: train\n",
      "[INFO] Number of images in the ETHMugDataset: 454\n",
      "[INFO] Dataset mode: test\n",
      "[INFO] Number of images in the ETHMugDataset: 300\n",
      "[INFO]: Saving the predicted segmentation masks to prediction\n",
      "[INFO]: Starting training...\n",
      "****************************\n",
      "0\n",
      "****************************\n",
      "Minibatch 0\n",
      "         Training Loss: 0.7111385464668274 - IoU: 0.04702584651310367\n",
      "Minibatch 1\n",
      "         Training Loss: 0.7005832195281982 - IoU: 0.06720006962119893\n",
      "Minibatch 2\n",
      "         Training Loss: 0.709017276763916 - IoU: 0.06346876204340747\n",
      "Minibatch 3\n",
      "         Training Loss: 0.703909695148468 - IoU: 0.06535358422379246\n",
      "Minibatch 4\n",
      "         Training Loss: 0.7033566236495972 - IoU: 0.07384160974940293\n",
      "Minibatch 5\n",
      "         Training Loss: 0.7042824625968933 - IoU: 0.04438038036271642\n",
      "Minibatch 6\n",
      "         Training Loss: 0.7097720503807068 - IoU: 0.05147732309289536\n",
      "Minibatch 7\n",
      "         Training Loss: 0.706000804901123 - IoU: 0.06500058096976791\n",
      "Minibatch 8\n",
      "         Training Loss: 0.7088123559951782 - IoU: 0.04675338369156006\n",
      "Minibatch 9\n",
      "         Training Loss: 0.699439525604248 - IoU: 0.04625479988240616\n",
      "Minibatch 10\n",
      "         Training Loss: 0.7047214508056641 - IoU: 0.03989175477612153\n",
      "Minibatch 11\n",
      "         Training Loss: 0.705803394317627 - IoU: 0.04697632241061452\n",
      "Minibatch 12\n",
      "         Training Loss: 0.7087637782096863 - IoU: 0.051804762056341554\n",
      "Minibatch 13\n",
      "         Training Loss: 0.7021624445915222 - IoU: 0.06344275849972204\n",
      "Minibatch 14\n",
      "         Training Loss: 0.7000759840011597 - IoU: 0.0489280553892113\n",
      "Minibatch 15\n",
      "         Training Loss: 0.7041180729866028 - IoU: 0.06563712302732988\n",
      "Minibatch 16\n",
      "         Training Loss: 0.6970935463905334 - IoU: 0.07112016025316203\n",
      "Minibatch 17\n",
      "         Training Loss: 0.7011356949806213 - IoU: 0.04971915054436167\n",
      "Minibatch 18\n",
      "         Training Loss: 0.7049402594566345 - IoU: 0.06215938014444566\n",
      "Minibatch 19\n",
      "         Training Loss: 0.6994221210479736 - IoU: 0.03763946875434022\n",
      "Minibatch 20\n",
      "         Training Loss: 0.7001554369926453 - IoU: 0.06689514611243033\n",
      "Minibatch 21\n",
      "         Training Loss: 0.7028530240058899 - IoU: 0.05551005059021958\n",
      "Minibatch 22\n",
      "         Training Loss: 0.7009496092796326 - IoU: 0.05874830809137287\n",
      "Minibatch 23\n",
      "         Training Loss: 0.700861930847168 - IoU: 0.0680669411249219\n",
      "Minibatch 24\n",
      "         Training Loss: 0.6998044848442078 - IoU: 0.05639923299814014\n",
      "Minibatch 25\n",
      "         Training Loss: 0.6997250914573669 - IoU: 0.058611850539758786\n",
      "Minibatch 26\n",
      "         Training Loss: 0.7033313512802124 - IoU: 0.045276287007994666\n",
      "Minibatch 27\n",
      "         Training Loss: 0.6996860504150391 - IoU: 0.034475654505346134\n",
      "Minibatch 28\n",
      "         Training Loss: 0.6945063471794128 - IoU: 0.0455431484167066\n",
      "Minibatch 29\n",
      "         Training Loss: 0.6958946585655212 - IoU: 0.056718686506006685\n",
      "Minibatch 30\n",
      "         Training Loss: 0.6962253451347351 - IoU: 0.06075751763071146\n",
      "Minibatch 31\n",
      "         Training Loss: 0.6987460255622864 - IoU: 0.06480616920774558\n",
      "Minibatch 32\n",
      "         Training Loss: 0.6993199586868286 - IoU: 0.06975964114119203\n",
      "Minibatch 33\n",
      "         Training Loss: 0.7007946968078613 - IoU: 0.04917901212439564\n",
      "Minibatch 34\n",
      "         Training Loss: 0.7024770975112915 - IoU: 0.051157821324659486\n",
      "Minibatch 35\n",
      "         Training Loss: 0.7021481990814209 - IoU: 0.043731843856412204\n",
      "Minibatch 36\n",
      "         Training Loss: 0.6971105933189392 - IoU: 0.054741968930163254\n",
      "Minibatch 37\n",
      "         Training Loss: 0.6981034874916077 - IoU: 0.0562571489225348\n",
      "Minibatch 38\n",
      "         Training Loss: 0.6970385909080505 - IoU: 0.06087272806491479\n",
      "Minibatch 39\n",
      "         Training Loss: 0.6978035569190979 - IoU: 0.03301564151205779\n",
      "Minibatch 40\n",
      "         Training Loss: 0.6980186104774475 - IoU: 0.027305681649669487\n",
      "Minibatch 41\n",
      "         Training Loss: 0.6977324485778809 - IoU: 0.0476176972558852\n",
      "Minibatch 42\n",
      "         Training Loss: 0.6976728439331055 - IoU: 0.04878853563566018\n",
      "Minibatch 43\n",
      "         Training Loss: 0.6996545791625977 - IoU: 0.06278752312543781\n",
      "Minibatch 44\n",
      "         Training Loss: 0.6972136497497559 - IoU: 0.0407966723945696\n",
      "Minibatch 45\n",
      "         Training Loss: 0.6922323703765869 - IoU: 0.059354241993723605\n",
      "Minibatch 46\n",
      "         Training Loss: 0.6958999633789062 - IoU: 0.04079861567924725\n",
      "Minibatch 47\n",
      "         Training Loss: 0.6948891878128052 - IoU: 0.05845098077704837\n",
      "Minibatch 48\n",
      "         Training Loss: 0.6932908296585083 - IoU: 0.04125229428010631\n",
      "Minibatch 49\n",
      "         Training Loss: 0.6940633654594421 - IoU: 0.07615075604436052\n",
      "Minibatch 50\n",
      "         Training Loss: 0.6930490136146545 - IoU: 0.0427698574365352\n",
      "Minibatch 51\n",
      "         Training Loss: 0.6942541599273682 - IoU: 0.07033056038599032\n",
      "Minibatch 52\n",
      "         Training Loss: 0.6939486265182495 - IoU: 0.05510100683694699\n",
      "Minibatch 53\n",
      "         Training Loss: 0.6948642730712891 - IoU: 0.059819021297597426\n",
      "Minibatch 54\n",
      "         Training Loss: 0.6933109760284424 - IoU: 0.05371922479983199\n",
      "Minibatch 55\n",
      "         Training Loss: 0.6909553408622742 - IoU: 0.043703942846192036\n",
      "Minibatch 56\n",
      "         Training Loss: 0.6901388168334961 - IoU: 0.0568946577170268\n",
      "****************************\n",
      "1\n",
      "****************************\n",
      "Minibatch 0\n",
      "         Training Loss: 0.6928209662437439 - IoU: 0.04008589633375221\n",
      "Minibatch 1\n",
      "         Training Loss: 0.6911863088607788 - IoU: 0.0723138820647584\n",
      "Minibatch 2\n",
      "         Training Loss: 0.69241863489151 - IoU: 0.034366346005926474\n",
      "Minibatch 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m test_data_root = os.path.join(data_root, \u001b[33m\"\u001b[39m\u001b[33mtest_data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO]: Test data root: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_data_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_unet_simplistic.py:89\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(ckpt_dir, train_data_root, test_data_root)\u001b[39m\n\u001b[32m     86\u001b[39m     optimizer.step()\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Trace output:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m         Training Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()),\n\u001b[32m     90\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33m- IoU: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(compute_iou(output.data.cpu().numpy() > \u001b[32m0.5\u001b[39m, gt_mask.data.cpu().numpy())))\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Save model (after every checkpoint)\u001b[39;00m\n\u001b[32m     93\u001b[39m torch.save(model.state_dict(), os.path.join(ckpt_dir, \u001b[33m\"\u001b[39m\u001b[33mlast_epoch.pth\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "ckpt_dir = os.path.join('./checkpoints', dt_string)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "print(\"[INFO]: Model checkpoints will be saved to:\", ckpt_dir)\n",
    "\n",
    "# Set data root\n",
    "data_root = 'sml_data/datasets'\n",
    "train_data_root = os.path.join(data_root, \"train_data\")\n",
    "print(f\"[INFO]: Train data root: {train_data_root}\")\n",
    "\n",
    "test_data_root = os.path.join(data_root, \"test_data\")\n",
    "print(f\"[INFO]: Test data root: {test_data_root}\")\n",
    "\n",
    "train(ckpt_dir, train_data_root, test_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899e220-d25d-4f23-a7ca-60acf6b13781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e94afd-f6eb-4051-9cdc-8565b8024b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
